<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Diffusion Models as a SDE | Qingyuan Jiang </title> <meta name="author" content="Qingyuan Jiang"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://qingyuan-jiang.github.io/blog/2025/diffusion/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Understanding Diffusion Models as a SDE",
            "description": "",
            "published": "February 16, 2025",
            "authors": [
              
              {
                "author": "Qingyuan Jiang",
                "authorURL": "https://Qingyuan-Jiang.github.io",
                "affiliations": [
                  {
                    "name": "RSN Lab",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Qingyuan</span> Jiang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding Diffusion Models as a SDE</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <p>As I’m learning diffusion models, I found these two blogs (<a href="https://yang-song.net/blog/2021/score/" rel="external nofollow noopener" target="_blank">Song</a> &amp; <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">Lil’Log</a>) help me a lot in addition to the original papers, from which this blog takes a huge reference as well. However, I find there are a little bit more details that can be discussed, especially on building the connection between DDPM and the corersponding SDE. Therefore I write this blog on top of the papers to help people learn DDPM beyond just adding noise and denoising.</p> <p>Also, I’m giving a detailed 1D mixture of Gaussian example as a toy example to build more intuitives, and visualize the diffusion process. With that we can also unveil some details analytically, and discuss the controlability of diffusion models.</p> <p>In this blog, we focus on summarizing the connect between the classical diffusion model (DDPM) and the score matching / stochastic differential equation (SDE). We’ll shortly describe each component separately in a high level (this may need some prior knowledge to both sides) Then we would introduce the connections in between. It would be much easier to understand (perhaps more helpful as well) if you pick up some basic knowledge on DDPM/score matching before.</p> <h2 id="ddpm-score-matching--sde">DDPM, score matching &amp; SDE</h2> <p>Diffusion models are first proposed as a generative model <d-cite key="sohl-dickstein_deep_2015"></d-cite> and were improved in <d-cite key="ho_denoising_2020"></d-cite>. Meanwhile, similar process were investigated independently from the score matching perspective <d-cite key="song_generative_2020"></d-cite>and <d-cite key="song_score-based_2021"></d-cite> <d-cite key="song_maximum_2021"></d-cite>.</p> <h3 id="diffusion-model-ddpm">Diffusion Model (DDPM)</h3> <p>The diffusion process is defined by iteratively adding Gaussian noises into the original data, where \(p(\mathbf{x}_{t+1}\mid\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t+1}; \sqrt{(1-\beta_t)} \mathbf{x}_t, \beta_t \mathbf{I})\), or as Eq.\eqref{eq: ddpm_pxt1x}. $\beta_t$ is a (designed) noise schedule. \(\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\). Typically, \(\beta_t\) is a linear function between, for example, \(\beta_{min}=0.001\), and \(\beta_{max}=0.02\). <d-footnote>In a few literatures, we differentiate the forward distribution with $q(\mathbf{x}_{t+1}\mid\mathbf{x}_t)$ and the reverse with $p(\mathbf{x}_{t+1}\mid\mathbf{x}_t)$. In this blog, since we are building the connection between other approaches, we simplify this notation by denote both with $p(\mathbf{x}_{t+1}\mid\mathbf{x}_t)$ and $p(\mathbf{x}_{t+1}\mid\mathbf{x}_t)$.</d-footnote></p> \[\begin{equation} \label{eq: ddpm_pxt1x} \mathbf{x}_{t+1} = \sqrt{(1-\beta_t)} \mathbf{x}_t + \sqrt{\beta_t} \epsilon \end{equation}\] <p>Applying a <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#what-are-diffusion-models" rel="external nofollow noopener" target="_blank">reparameterization trick</a>, we can obtain the distribution over time as Eq. \eqref{eq: ddpm_pdf}. Note that we define \(\alpha_t = 1-\beta_t\), \(\bar{\alpha}_t = \prod_{i=0}^t \alpha_i\).</p> \[\begin{equation} \label{eq: ddpm_pdf} p(\mathbf{x}_t\mid\mathbf{x}_0) = \mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I}) \end{equation}\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM.png" class="img-fluid rounded z-depth-1" alt="DDPM"> </div> </div> <div class="caption"> Illustration for diffusion process (DDPM). Source image from <d-cite key="ho_denoising_2020"></d-cite>. </div> <p>In the sampling time, we reverse the diffusion process by iteratively denoise with a parameterized (trained) probability \(p_\theta(\mathbf{x}_{t-1}\mid\mathbf{x}_t)=\mathcal{N}(\mathbf{x}_{t-1}; \mathbf{\mu}_\theta, \mathbf{\Sigma}_\theta \mathbf{I})\). We sample \(\mathbf{x}_t\) from a zero-mean Gaussian \(\mathcal{N}(\mathbf{0}, \mathbf{I})\), and denoise in a Markov Chain with such Gaussian noise until we get \(\mathbf{x}_0\).</p> <h3 id="score-matching">Score Matching</h3> <p>The diffusion process can be also viewed as a score matching process, where a score function is defined as the \(\nabla_\mathbf{x} \log p(\mathbf{x})\). Score function describe the gradient towar\mathrm{d}s the high probability area in the manifold. We can learn this function with a neural network \(s_\theta (\mathbf{x})\) by minimizing the fisher divergence \(\mathbb{D}_F = \mathbb{E}_{x \sim p_{data}(\mathbf{x})}\|\nabla_\mathbf{x} \log p_{data}(\mathbf{x}) - s_\theta (\mathbf{x}) \|^2_2\). <d-footnote>In practice, we do not have access to the real score function $\nabla_\mathbf{x} \log p_{data}(\mathbf{x})$ from the data. Therefore, a few metho\mathrm{d}s <d-cite key="vincent_connection_2011"></d-cite><d-cite key="song_sliced_2019"></d-cite><d-cite key="song_generative_2020"></d-cite>are designed to help learn such a score function from the data. </d-footnote></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://yang-song.net/assets/img/score/smld.jpg" class="img-fluid rounded z-depth-1" alt="DDPM"> </div> </div> <div class="caption"> Illustration for diffusion process (DDPM). Source image from <d-cite key="ho_denoising_2020"></d-cite>. </div> <p>The sampling process, in this context, is formulated as a <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="external nofollow noopener" target="_blank">Markov Chain Monte Carlo (MCMC) process</a> based on the Langevin dynamics, which is a concept from physics used for statistically modeling the molecular system. The sampling process is defined as Eq.\eqref{eq: smld}, with a step size \(\delta\) and a Gaussian noise \(\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\).</p> \[\begin{equation} \label{eq: smld} \mathbf{x}_{t+1} = \mathbf{x}_t + \delta \nabla_\mathbf{x} \log p(\mathbf{x}) + \sqrt{2\delta} \epsilon_t \end{equation}\] <h3 id="stochastic-differentiable-equation-sde">Stochastic Differentiable Equation (SDE)</h3> <p>Both DDPM and score matching can be formulated into a unified stocastic differentiable equation (SDE) framework, where the (reverse) diffusion process are generalized as a continuous process. A general form of a SDE is usually written as \(\mathrm{d}\mathbf{x} = f(\mathbf{x},t)\mathrm{d}t + g(\mathbf{x},t)\mathrm{d}w\). \(f(\mathbf{x},t)\) is the shifting term, and \(\mathrm{d}w\) is a random noise.</p> <p>In the diffusion model context, this SDE can be designed different to describe different diffusion method. For example, in score matching perspective, an simplest example can be \(\mathrm{d}\mathbf{x} = e^t \mathrm{d}w\). This is same as perturbing the data with \(\mathcal{N}(\mathbf{0}, \sigma_1^2 \mathbf{I}), \mathcal{N}(\mathbf{0}, \sigma_2^2 \mathbf{I}), \ldots, \mathcal{N}(\mathbf{0}, \sigma_t^2 \mathbf{I})\), i.e., a sequence of zero-mean Gaussians with exponential growing variance. Or, it can be designed as the DDPM algorithm, which describe the SDE process as Eq.\eqref{eq: ddpm_as_sde}.</p> \[\begin{equation} \label{eq: ddpm_as_sde} \mathrm{d}\mathbf{x} = -\frac{1}{2} \beta(t) \mathbf{x} \mathrm{d}t + \sqrt{\beta(t)} \mathrm{d}w \end{equation}\] <p>Any SDE has a corresponding reverse SDE as Eq.\eqref{eq: general_reverse_sde}, in which \(\mathrm{d}t\) is a negative infinitesimal time step, and the score function can be learnt by \(s_\theta (\mathbf{x})\).</p> \[\begin{equation} \label{eq: general_reverse_sde} \mathrm{d}\mathbf{x} = \left[ f(\mathbf{x}, t) - g^2(t) \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] \mathrm{d}t + g(t) \mathrm{d}w \end{equation}\] <p>In practice, we can use Euler-Maruyama method to solve the SDE in a descritized way.</p> \[\label{eq: euler-maruyama} \begin{aligned} \Delta \mathbf{x} &amp;\leftarrow \left[ f(\mathbf{x}, t) - g^2(t) \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] \mathrm{d}t + g(t) \sqrt{\mid\Delta t\mid} \epsilon \\ \mathbf{x} &amp;\leftarrow \mathbf{x} + \Delta \mathbf{x} \\ t &amp;\leftarrow t + \Delta t \end{aligned}\] <p>The marginal probability density function can be calculated by solving the SDE. In the DDPM algorithm, more specifically, the corresponding ordinary differentiable equation (ODE) can be computed as below.</p> \[\begin{aligned} \mathrm{d}\mathbf{x} &amp;= -\frac{1}{2} \beta_t \mathbf{x} \mathrm{d}t &amp; \text{(math)} \\ \int_{\mathbf{x}_0}^{\mathbf{x}_t} \frac{1}{x} \mathrm{d}\mathbf{x} &amp;= \int_0^t -\frac{1}{2} \beta_s \mathrm{d}s &amp; \text{(integral)} \\ \log \frac{\mathbf{x}_t}{\mathbf{x}_0} &amp;= \int_0^t -\frac{1}{2} \beta_s \mathrm{d}s &amp; \text{(integral)} \\ \mathbf{x}_t &amp;= e^{-\frac{1}{2} \int_0^t \beta_s \mathrm{d}s} \mathbf{x}_0 &amp; \text{(exponential)} \end{aligned}\] <p>The calculation of the variance of the marginal PDF can be found from <d-cite key="song_score-based_2021"></d-cite>. The overall probability density function is as follows</p> \[\begin{equation} \label{eq: ddpm_sde_pdf} p(\mathbf{x}_t\mid\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; e^{-\frac{1}{2} \int_0^t \beta(s) \mathrm{d}s} \mathbf{x}_0, [1 - e^{-\int_0^t \beta(s) \mathrm{d}s}] \mathbf{I}) \end{equation}\] <h2 id="ddpm-as-a-sde">DDPM as a SDE</h2> <p>So far we’ve introduced the basic concept of DDPM, score matching, and diffusion as a SDE. In this section, we show they are equivalent in the following perspectives.</p> <h3 id="gaussian-kernel">Gaussian Kernel</h3> <p>In the DDPM, we add a Gaussian noise with a noise schedule as Eq.\eqref{eq: ddpm_pxt1x}. Here we show this is consistent with the SDE formulation in Eq.\eqref{eq: ddpm_as_sde}.</p> <p>Note that in this specific section, we need to differentiate the noise we use in SDE, denoted by \(\beta(t)\), and the noise from original DDPM \(\beta_t\). They have a scale relationship in between, \(\beta_t = \beta(t) \Delta t\) so that they contribute equally when applying the integral.</p> \[\begin{aligned} \mathbf{x}_{t+1} &amp;= \sqrt{(1-\beta_t)} \mathbf{x}_t + \sqrt{\beta_t} \epsilon &amp; \text{(DDPM)} \\ \mathbf{x}(t+\Delta t) &amp;= \sqrt{(1-\beta(t)\Delta t)} \mathbf{x}(t) + \sqrt{\beta(t) \Delta t} \epsilon &amp; \text{(discrete to continuous)} \\ \mathbf{x}(t+\Delta t) &amp;= (1 + \frac{1}{2}\frac{-\beta(t)\Delta t}{\sqrt{(1-\beta(t)\Delta t)}}) \mathbf{x}(t) + \sqrt{\beta(t) \Delta t} \epsilon &amp; \text{(Taylor Expansion)} \\ \mathbf{x}(t+\Delta t) &amp;= (1 + \frac{1}{2}\frac{-\beta(t)\Delta t}{1}) \mathbf{x}(t) + \sqrt{\beta(t) \Delta t} \epsilon &amp; (\beta(t)\Delta t &lt;&lt; 1) \\ \mathbf{x}(t+\Delta t) - \mathbf{x}(t) &amp;= - \frac{1}{2}\beta(t)\Delta t \mathbf{x}(t) + \sqrt{\beta(t) \Delta t} \epsilon &amp; \text{(re-write)} \\ \mathrm{d}\mathbf{x} &amp;= - \frac{1}{2}\beta(t) \mathbf{x}(t) \mathrm{d}t + \sqrt{\beta(t)} \mathrm{d}w &amp; \text{(VP SDE)} \end{aligned}\] <p>This way we show that the Gaussian kernel applied to the DDPM process is equiavalent to the variance preserving (VP) SDE defined in <d-cite key="song_score-based_2021"></d-cite>.</p> <h3 id="probability-density-function">Probability Density Function</h3> <p>We can also compare the probability density function from both DDPM and SDE perspectives. In DDPM, we describe the PDF as \(p(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha_t}} \mathbf{x}_t, (1-\bar{\alpha_t})\mathbf{I})\) in Eq.~\eqref{eq: ddpm_pdf}. In SDE, we previously show \(p(\mathbf{x}(t) \mid \mathbf{x}(0)) = \mathcal{N}(\mathbf{x}(t); e^{-\frac{1}{2} \int_0^t \beta(s) \mathrm{d}s} \mathbf{x}(0), [1 - e^{-\int_0^t \beta(s) \mathrm{d}s}] \mathbf{I})\) in Eq.\eqref{eq: ddpm_sde_pdf}.</p> <p>Here we’ll first show that \(\sqrt{\bar{\alpha_t}}\) is a good approximation of \(e^{-\frac{1}{2} \int_0^t \beta(s) \mathrm{d}s}\). Again, \(\beta_t = \beta(t) \Delta t\).</p> \[\begin{aligned} \ln(1-\beta_i) &amp;\simeq -\beta_i &amp; (\beta_i \ll 1, \ln x \simeq (x-1) \text{ when } x \simeq 1) \\ \ln \prod_{i=1}^t (1-\beta_i) &amp;\simeq -\sum_{i=1}^t \beta_i &amp; (\text{logarithm}) \\ \prod_{i=1}^t (1-\beta_i) &amp;\simeq \exp \left( -\sum_{i=1}^t \beta_i \right) &amp; (\text{exponentiating}) \\ \bar{\alpha_t} &amp;\simeq \exp \left( -\sum_{i=1}^t \beta_i \right) &amp; (\text{definition}) \\ \sqrt{\bar{\alpha_t}} &amp;\simeq \exp \left( -\frac{1}{2} \sum_{i=1}^t \beta_i \right) &amp; (\text{square root}) \\ \sqrt{\bar{\alpha_t}} &amp;\simeq \exp \left( -\frac{1}{2} \int_0^t \beta(s) \mathrm{d}s \right) &amp; (\text{Riemann sum}) \end{aligned}\] <p>For the variance, we have</p> \[\begin{aligned} \sqrt{\bar{\alpha_t}} &amp;\simeq \exp \left( -\frac{1}{2} \int_0^t \beta(s) \mathrm{d}s \right) &amp; \\ \bar{\alpha_t} &amp;\simeq \left( \exp (-\frac{1}{2} \int_0^t \beta(s) \mathrm{d}s ) \right)^2 &amp; (\text{square}) \\ 1 - \bar{\alpha_t} &amp;\simeq 1 - e^{-\int_0^t \beta_s \mathrm{d}s} &amp; (\text{math}) \end{aligned}\] <h3 id="reverse-process">Reverse Process</h3> <p>The DDPM reverse process is defined by</p> \[\begin{equation} \label{eq: ddpm_denoise} \begin{split} \mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} (\mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha_t}}} \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c})) + \sqrt{\bar{\beta_t}} \epsilon \\ \end{split} \end{equation}\] <p>Where the reverse SDE process for DDPM can be written as \refeq{eq: ddpm_reverse_sde}</p> \[\begin{equation} \label{eq: ddpm_reverse_sde} \begin{split} \mathrm{d}\mathbf{x} &amp;= \left[ f(\mathbf{x}, t) - g^2(t) \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] \mathrm{d}t + g(t) \mathrm{d}w \\ &amp;= \left[ -\frac{1}{2} \beta(t) \mathbf{x} - \beta(t) \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] \mathrm{d}t + \sqrt{\beta_t} \mathrm{d}w \end{split} \end{equation}\] <h2 id="example-1d-mixture-of-gaussian">Example: 1D Mixture of Gaussian</h2> <p>Let’s consider a 1D diffusion process so that we have a more direct intuitive, also we have some analytical result. Define the data distribution we are interested in as a mixture of Gaussian (MoG), \(p(x_0) = \sum_{i=\{1, 2\}} \pi_i \mathcal{N}(\mu_i, \sigma_i^2)\). <d-footnote>Since the data is 1-dimensional, we use $x$ to denote the scalar, instead of using $\mathbf{x}$ as a vector. So as $\mu$ and $\sigma$.</d-footnote> Given the distribution, we can sample a set of samples \(\{x_i\}\). We can diffuse them following the SDE process towards a Gaussian distribution.</p> <p>Notably, since we have the original data distribution, we can obtain the marginal probability density function $p(x_t)$ in the diffusion process. It will be another Gaussian, which we can compute the mean and the variance separately. For a single Gaussian component, denote \(p(x_t\mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, \sigma^2_t)\), where \(\bar{\alpha}_t = e^{-\int_0^t \beta_s \mathrm{d}s}\), \(\sigma_t = \sqrt{1 - \alpha_t}\)<d-footnote>This is consistent with DDPM convention.</d-footnote>, we have</p> \[\begin{equation} \label{eq: mog_mpdf} \begin{split} \mathbb{E}_i[x_t] &amp;= \mathbb{E}_{x_0} \left[\mathbb{E}_{x_t \sim p(x_t\mid x_0)} x_t \right] = \mathbb{E}_{x_0} \left[\sqrt{\bar{\alpha}_t} \mathbf{x}_0 \right] = \sqrt{\bar{\alpha}_t} \mu_i \\ \mathrm{Var}_i[x_t] &amp;= \mathbb{E}[\mathrm{Var}(x_t\mid x_0)] + \mathrm{Var}(\mathbb{E}[x_t\mid x_0]) = \sigma^2_t + \bar{\alpha}_t \sigma^2_i \\ p(x_t) &amp;= \sum_{i=\{1, 2\}} \pi_i \mathcal{N}\left( x_t; \sqrt{\bar{\alpha}_t} \mu_i, \sigma^2_t + \bar{\alpha}_t \sigma^2_i \right) \end{split} \end{equation}\] <p>Since we have the analytical solution to this probability density function, we can write the score function as well.</p> \[\begin{equation} \label{eq: mog_sf} \begin{split} \nabla_x \log p(x_t) &amp;= \nabla_x \log \sum_{i=\{1, 2\}} \pi_i \mathcal{N}\left( x_t; \sqrt{\bar{\alpha}_t} \mu_i, \sigma^2_t + \bar{\alpha}_t \sigma^2_i \right) \\ &amp;= \nabla_x \log \sum_{i=\{1, 2\}} \pi_i \frac{1}{\sqrt{2\pi (\sigma^2_t + \bar{\alpha}_t \sigma^2_i)}} \exp \left( -\frac{(x_t - \sqrt{\bar{\alpha}_t} \mu_i)^2}{2 (\sigma^2_t + \bar{\alpha}_t \sigma^2_i)} \right) \\ &amp;= \frac{\sum_{i=\{1, 2\}} \pi_i \frac{1}{\sqrt{2\pi (\sigma^2_t + \bar{\alpha}_t \sigma^2_i)}} \exp \left( -\frac{(x_t - \sqrt{\bar{\alpha}_t} \mu_i)^2}{2 (\sigma^2_t + \bar{\alpha}_t \sigma^2_i)} \right) \left( -\frac{(x_t - \sqrt{\bar{\alpha}_t} \mu_i)}{(\sigma^2_t + \bar{\alpha}_t \sigma^2_i)} \right)}{p(x_t)} \\ &amp;= \sum_{i=\{1, 2\}} \frac{\pi_i \mathcal{N}\left( x_t; \sqrt{\bar{\alpha}_t} \mu_i, \sigma^2_t + \bar{\alpha}_t \sigma^2_i \right)}{p(x_t)} \left( -\frac{(x_t - \sqrt{\bar{\alpha}_t} \mu_i)}{(\sigma^2_t + \bar{\alpha}_t \sigma^2_i)} \right) \\ &amp;= \sum_{i=\{1, 2\}} \gamma_i \left( -\frac{(x_t - \sqrt{\bar{\alpha}_t} \mu_i)}{(\sigma^2_t + \bar{\alpha}_t \sigma^2_i)} \right) \end{split} \end{equation}\] <p>where \(\gamma_i = \frac{\pi_i \mathcal{N}\left( x_t; \sqrt{\bar{\alpha}_t} \mu_i, \sigma^2_t + \bar{\alpha}_t \sigma^2_i \right)}{p(x_t)}\) is the probability portion (weight) for a Gaussian component.</p> <p>If we use a linear noise schedule, \(\beta_t = \beta_{min} + t (\beta_{max} - \beta{min})\), we can either obtain the close-form solution for \(\bar{\alpha}_t = e^{-(\frac{1}{2}t^2(\beta_{max} - \beta_{min}) + t \beta_{min})}\), or use Riemann approximation \(\bar{\alpha}_t = e^{-\sum(\beta_t \Delta t)}\) (more generalized method).</p> <p>We visualize this analytical solution as the figure below.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-diffusion/sde-480.webp 480w,/assets/img/posts/2025-02-16-diffusion/sde-800.webp 800w,/assets/img/posts/2025-02-16-diffusion/sde-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-02-16-diffusion/sde.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 1D diffusion process from a mixture of Gaussian (left) to a zero-mean Gaussian (right). Each line represents one sample and its corresponding diffusion process. Background represents the probability density function at each time step. Brighter area represents higher probability. White line represents the corresponding ODE process. </div> <h2 id="conditioned-generation">Conditioned Generation</h2> <p>One advantage of diffusion model is its <strong>controlability</strong> using the conditioned diffusion generation process. It was originally described as the <code class="language-plaintext highlighter-rouge">perturbed gaussian transition</code> in <d-cite key="sohl-dickstein_deep_2015"></d-cite>, and was later used in classified image generation in <d-cite key="nichol_glide_2022, ho_classifier-free_2022"></d-cite>.</p> <p>Considering the following joint probability \(p(\mathbf{x}, y)\), where \(y\) can be a label/caption to an image. Applying the Bayesian rule, we have</p> \[\begin{equation} \label{eq: sm_joint_prob} \begin{split} p(\mathbf{x}_t, y) &amp;= p(\mathbf{x}_t) p(y\mid\mathbf{x}_t) \\ \nabla_{\mathbf{x}} \log p(\mathbf{x}_t, y) &amp;= \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) + \nabla_{\mathbf{x}} \log p(y\mid\mathbf{x}_t) \\ \end{split} \end{equation}\] <p>If we have an explicit classifier \(p(y \mid \mathbf{x}) = f_\phi(y \mid \mathbf{x}_t, t)\), in the DDPM context, we can use the gradient to guide the diffusion sampling process.</p> \[\begin{equation*} \begin{split} \nabla_{\mathbf{x}} \log p(\mathbf{x}_t, y) &amp;= \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) + \nabla_{\mathbf{x}} \log p(y\mid\mathbf{x}_t) \\ &amp;= \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) + \nabla_{\mathbf{x}} \log f_\phi(y\mid\mathbf{x}_t, t) \\ \end{split} \end{equation*}\] <h3 id="guided-1d-mog-diffusion">Guided 1D MoG Diffusion</h3> <p>In our 1D MoG example, if we use \(p(y \mid x) = \mathcal{N}(\mu_i; \sigma^2_i)\) as our guidance, we are leading our denoising into one of the Gaussian component.</p> \[\begin{equation*} \nabla_{x} \log p(y \mid x_t) = \nabla_{x} \log \mathcal{N}(\mu_i; \sigma^2_i) = - \frac{x-\mu_i}{\sigma^2_i} \end{equation*}\] \[\begin{equation*} \begin{split} \nabla_{x} \log p(x_t, y) &amp;= \nabla_{x} \log p(x_t) + \nabla_{x} \log p(y\mid x_t) \\ &amp;= \sum_{i=\{1, 2\}} \gamma_i \left( -\frac{(x_t - \sqrt{\bar{\alpha}_t} \mu_i)}{(\sigma^2_t + \bar{\alpha}_t \sigma^2_i)} \right) - \frac{x-\mu_i}{\sigma^2_i} \end{split} \end{equation*}\] <p>This way, as shown in the visualization below, all the samples are guided into one of the Gaussian components in the reverse SDE process.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-02-16-diffusion/sde_perturb-480.webp 480w,/assets/img/posts/2025-02-16-diffusion/sde_perturb-800.webp 800w,/assets/img/posts/2025-02-16-diffusion/sde_perturb-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-02-16-diffusion/sde_perturb.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Guided diffusion process towards one of the MoG Gaussian component. </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-02-16-diffusion.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Qingyuan Jiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-understanding-diffusion-models-as-a-sde",title:"Understanding Diffusion Models as a SDE",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/diffusion/"}},{id:"post-my-ta-note-3dcv-triangulation",title:"My TA Note - 3DCV - Triangulation",description:"Notes for TAing the Computer Vision course (F2024).",section:"Posts",handler:()=>{window.location.href="/assets/pdf/study_notes/triangulation.pdf"}},{id:"post-my-ta-note-3dcv-epipolar-geometry",title:"My TA Note - 3DCV - Epipolar Geometry",description:"Notes for TAing the Computer Vision course (F2024).",section:"Posts",handler:()=>{window.location.href="/assets/pdf/study_notes/epipolar_geometry.pdf"}},{id:"post-my-ta-note-cv-transformation-amp-camera-models",title:"My TA Note - CV - Transformation & Camera Models",description:"Notes for TAing the Computer Vision course (F2024).",section:"Posts",handler:()=>{window.location.href="/assets/pdf/study_notes/transformation.pdf"}},{id:"news-start-my-ph-d-in-the-university-of-minnesota-sparkles-smile",title:'Start my Ph.D. in the University of Minnesota. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-start-my-internship-at-plus-ai-working-on-reinforcement-learning-for-autonomous-driving-control",title:"Start my internship at Plus.AI, working on Reinforcement Learning for autonomous driving control....",description:"",section:"News"},{id:"projects-closed-loop-diffusion-planning",title:"Closed-loop Diffusion Planning",description:"Diffusion-based Robot planning with Recursive Bayesian Filter",section:"Projects",handler:()=>{window.location.href="/projects/cldp/"}},{id:"projects-human-pose-forecasting",title:"Human Pose Forecasting",description:"Map-Aware Human Pose Prediction for Robot Follow-Ahead",section:"Projects",handler:()=>{window.location.href="/projects/rfa/"}},{id:"projects-sportsvision",title:"SportsVision",description:"Automatic highlight extraction for basketball games",section:"Projects",handler:()=>{window.location.href="/projects/sportsvision/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%66%6F%78%69%6A%69%61%6E%67%31%35@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Qingyuan-Jiang","_blank")}},{id:"social-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=ob-N2PEAAAAJ","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>