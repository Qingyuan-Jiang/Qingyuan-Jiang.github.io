
@inproceedings{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}.},
	pages = {6840--6851},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2023-12-03},
	date = {2020},
	file = {Full Text PDF:C\:\\Users\\foxij\\Zotero\\storage\\K52A76AV\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@misc{song_denoising_2022,
	title = {Denoising Diffusion Implicit Models},
	url = {http://arxiv.org/abs/2010.02502},
	abstract = {Denoising diffusion probabilistic models ({DDPMs}) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models ({DDIMs}), a more efﬁcient class of iterative implicit probabilistic models with the same training procedure as {DDPMs}. In {DDPMs}, the generative process is deﬁned as the reverse of a particular Markovian diffusion process. We generalize {DDPMs} via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that {DDIMs} can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to {DDPMs}, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.},
	number = {{arXiv}:2010.02502},
	publisher = {{arXiv}},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	urldate = {2023-12-03},
	date = {2022-10-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.02502 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Song et al. - 2022 - Denoising Diffusion Implicit Models.pdf:C\:\\Users\\foxij\\Zotero\\storage\\5YL37PXC\\Song et al. - 2022 - Denoising Diffusion Implicit Models.pdf:application/pdf},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
	url = {http://arxiv.org/abs/1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	number = {{arXiv}:1503.03585},
	publisher = {{arXiv}},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	urldate = {2024-02-06},
	date = {2015-11-18},
	eprinttype = {arxiv},
	eprint = {1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\QBYMYP4P\\1503.html:text/html;Full Text PDF:C\:\\Users\\foxij\\Zotero\\storage\\PJFKBC77\\Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf},
}

@misc{rombach_high-resolution_2022,
	title = {High-Resolution Image Synthesis with Latent Diffusion Models},
	url = {http://arxiv.org/abs/2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models ({DMs}) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful {DMs} often consumes hundreds of {GPU} days and inference is expensive due to sequential evaluations. To enable {DM} training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models ({LDMs}) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based {DMs}. Code is available at https://github.com/{CompVis}/latent-diffusion .},
	number = {{arXiv}:2112.10752},
	publisher = {{arXiv}},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	urldate = {2024-09-24},
	date = {2022-04-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:C\:\\Users\\foxij\\Zotero\\storage\\Y7D5RAZC\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf},
}

@misc{song_how_2021,
	title = {How to Train Your Energy-Based Models},
	url = {http://arxiv.org/abs/2101.03288},
	abstract = {Energy-Based Models ({EBMs}), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, {EBMs} do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of {EBMs} makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for {EBM} training. We start by explaining maximum likelihood training with Markov chain Monte Carlo ({MCMC}), and proceed to elaborate on {MCMC}-free approaches, including Score Matching ({SM}) and Noise Constrastive Estimation ({NCE}). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply {EBMs} or start a research project in this direction.},
	number = {{arXiv}:2101.03288},
	publisher = {{arXiv}},
	author = {Song, Yang and Kingma, Diederik P.},
	urldate = {2024-11-19},
	date = {2021-02-17},
	eprinttype = {arxiv},
	eprint = {2101.03288},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\BAPBZ5SX\\Song and Kingma - 2021 - How to Train Your Energy-Based Models.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\P4VLEWX9\\2101.html:text/html},
}

@misc{song_generative_2020,
	title = {Generative Modeling by Estimating Gradients of the Data Distribution},
	url = {http://arxiv.org/abs/1907.05600},
	doi = {10.48550/arXiv.1907.05600},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to {GANs} on {MNIST}, {CelebA} and {CIFAR}-10 datasets, achieving a new state-of-the-art inception score of 8.87 on {CIFAR}-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	number = {{arXiv}:1907.05600},
	publisher = {{arXiv}},
	author = {Song, Yang and Ermon, Stefano},
	urldate = {2025-02-07},
	date = {2020-10-10},
	eprinttype = {arxiv},
	eprint = {1907.05600 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\V23JTWWK\\Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\M9BN4CRQ\\1907.html:text/html},
}

@article{chi_diffusion_nodate,
	title = {Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
	author = {Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
	langid = {english},
	file = {Chi et al. - Diffusion Policy Visuomotor Policy Learning via A.pdf:C\:\\Users\\foxij\\Zotero\\storage\\LCE5X6AA\\Chi et al. - Diffusion Policy Visuomotor Policy Learning via A.pdf:application/pdf},
}

@misc{song_score-based_2021,
	title = {Score-Based Generative Modeling through Stochastic Differential Equations},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation ({SDE}) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time {SDE} that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time {SDE} depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical {SDE} solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time {SDE}. We also derive an equivalent neural {ODE} that samples from the same distribution as the {SDE}, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on {CIFAR}-10 with an Inception score of 9.89 and {FID} of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	number = {{arXiv}:2011.13456},
	publisher = {{arXiv}},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	urldate = {2025-02-07},
	date = {2021-02-10},
	eprinttype = {arxiv},
	eprint = {2011.13456 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\Z4BT3G7J\\Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\8C9XTQL8\\2011.html:text/html},
}

@article{vincent_connection_2011,
	title = {A Connection Between Score Matching and Denoising Autoencoders},
	volume = {23},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/23/7/1661-1674/7677},
	doi = {10.1162/NECO_a_00142},
	abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to Restricted Boltzmann Machines for unsupervised pre-training of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a speciﬁc energy based model to that of a non-parametric Parzen density estimator of the data. This yields several useful insights. It deﬁnes a proper probabilistic model for the denoising autoencoder technique which makes it in principle possible to sample from them or to rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justiﬁes the use of tied weights between the encoder and decoder, and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
	pages = {1661--1674},
	number = {7},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Vincent, Pascal},
	urldate = {2025-02-07},
	date = {2011-07},
	langid = {english},
	file = {Vincent - 2011 - A Connection Between Score Matching and Denoising .pdf:C\:\\Users\\foxij\\Zotero\\storage\\QMSUBB8P\\Vincent - 2011 - A Connection Between Score Matching and Denoising .pdf:application/pdf},
}

@misc{song_maximum_2021,
	title = {Maximum Likelihood Training of Score-Based Diffusion Models},
	url = {http://arxiv.org/abs/2101.09258},
	doi = {10.48550/arXiv.2101.09258},
	abstract = {Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on {CIFAR}-10 and {ImageNet} 32x32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.},
	number = {{arXiv}:2101.09258},
	publisher = {{arXiv}},
	author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
	urldate = {2025-02-07},
	date = {2021-10-21},
	eprinttype = {arxiv},
	eprint = {2101.09258 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\HTPCRG84\\Song et al. - 2021 - Maximum Likelihood Training of Score-Based Diffusi.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\E3892JXD\\2101.html:text/html},
}

@article{hyvarinen_estimation_nodate,
	title = {Estimation of Non-Normalized Statistical Models by Score Matching},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data.},
	author = {Hyvarinen, Aapo},
	langid = {english},
	file = {Hyvarinen - Estimation of Non-Normalized Statistical Models by.pdf:C\:\\Users\\foxij\\Zotero\\storage\\B6FI32PL\\Hyvarinen - Estimation of Non-Normalized Statistical Models by.pdf:application/pdf},
}

@misc{ho_classifier-free_2022,
	title = {Classifier-Free Diffusion Guidance},
	url = {http://arxiv.org/abs/2207.12598},
	doi = {10.48550/arXiv.2207.12598},
	abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
	number = {{arXiv}:2207.12598},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Salimans, Tim},
	urldate = {2025-02-07},
	date = {2022-07-26},
	eprinttype = {arxiv},
	eprint = {2207.12598 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\PB35RTI6\\Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\D2LDLM4J\\2207.html:text/html},
}

@misc{lipman_flow_2023,
	title = {Flow Matching for Generative Modeling},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows ({CNFs}), allowing us to train {CNFs} at unprecedented scale. Specifically, we present the notion of Flow Matching ({FM}), a simulation-free approach for training {CNFs} based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing {FM} with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training {CNFs} with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport ({OT}) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training {CNFs} using Flow Matching on {ImageNet} leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical {ODE} solvers.},
	number = {{arXiv}:2210.02747},
	publisher = {{arXiv}},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	urldate = {2025-02-07},
	date = {2023-02-08},
	eprinttype = {arxiv},
	eprint = {2210.02747 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\3SEEERLV\\Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\4QDRYD5P\\2210.html:text/html},
}

@misc{hong_lrm_2024,
	title = {{LRM}: Large Reconstruction Model for Single Image to 3D},
	url = {http://arxiv.org/abs/2311.04400},
	doi = {10.48550/arXiv.2311.04400},
	shorttitle = {{LRM}},
	abstract = {We propose the first Large Reconstruction Model ({LRM}) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as {ShapeNet} in a category-specific fashion, {LRM} adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field ({NeRF}) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from {MVImgNet}. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our {LRM} project webpage: https://yiconghong.me/{LRM}.},
	number = {{arXiv}:2311.04400},
	publisher = {{arXiv}},
	author = {Hong, Yicong and Zhang, Kai and Gu, Jiuxiang and Bi, Sai and Zhou, Yang and Liu, Difan and Liu, Feng and Sunkavalli, Kalyan and Bui, Trung and Tan, Hao},
	urldate = {2025-02-07},
	date = {2024-03-09},
	eprinttype = {arxiv},
	eprint = {2311.04400 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\V7KB8PTN\\Hong et al. - 2024 - LRM Large Reconstruction Model for Single Image t.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\RIUCDBRJ\\2311.html:text/html},
}

@misc{yang_learning_2024,
	title = {Learning Interactive Real-World Simulators},
	url = {http://arxiv.org/abs/2310.06114},
	doi = {10.48550/arXiv.2310.06114},
	abstract = {Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator ({UniSim}) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different dimensions (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, we can simulate the visual outcome of both high-level instructions such as "open the drawer" and low-level controls from otherwise static scenes and objects. We use the simulator to train both high-level vision-language policies and low-level reinforcement learning policies, each of which can be deployed in the real world in zero shot after training purely in simulation. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience, opening up even wider applications. Video demos can be found at https://universal-simulator.github.io.},
	number = {{arXiv}:2310.06114},
	publisher = {{arXiv}},
	author = {Yang, Sherry and Du, Yilun and Ghasemipour, Kamyar and Tompson, Jonathan and Kaelbling, Leslie and Schuurmans, Dale and Abbeel, Pieter},
	urldate = {2025-02-07},
	date = {2024-09-26},
	eprinttype = {arxiv},
	eprint = {2310.06114 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\AZXFSPYX\\Yang et al. - 2024 - Learning Interactive Real-World Simulators.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\Q2QFLPZI\\2310.html:text/html},
}

@misc{driess_palm-e_2023,
	title = {{PaLM}-E: An Embodied Multimodal Language Model},
	url = {http://arxiv.org/abs/2303.03378},
	doi = {10.48550/arXiv.2303.03378},
	shorttitle = {{PaLM}-E},
	abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that {PaLM}-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, {PaLM}-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on {OK}-{VQA}, and retains generalist language capabilities with increasing scale.},
	number = {{arXiv}:2303.03378},
	publisher = {{arXiv}},
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	urldate = {2025-02-07},
	date = {2023-03-06},
	eprinttype = {arxiv},
	eprint = {2303.03378 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\HJ8BETNI\\Driess et al. - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\84LRSPHC\\2303.html:text/html},
}

@misc{xu_geometric_2023,
	title = {Geometric Latent Diffusion Models for 3D Molecule Generation},
	url = {http://arxiv.org/abs/2305.01140},
	doi = {10.48550/arXiv.2305.01140},
	abstract = {Generative models, especially diffusion models ({DMs}), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models ({GeoLDM}). {GeoLDM} is the first latent {DM} model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and {DMs} operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that {GeoLDM} can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7{\textbackslash}\% improvement for the valid percentage of large biomolecules. Results also demonstrate {GeoLDM}'s higher capacity for controllable generation thanks to the latent modeling. Code is provided at {\textbackslash}url\{https://github.com/{MinkaiXu}/{GeoLDM}\}.},
	number = {{arXiv}:2305.01140},
	publisher = {{arXiv}},
	author = {Xu, Minkai and Powers, Alexander and Dror, Ron and Ermon, Stefano and Leskovec, Jure},
	urldate = {2025-02-07},
	date = {2023-05-02},
	eprinttype = {arxiv},
	eprint = {2305.01140 [cs]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\VT5K99ZA\\Xu et al. - 2023 - Geometric Latent Diffusion Models for 3D Molecule .pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\AVW6ISVP\\2305.html:text/html},
}

@article{lecun_tutorial_nodate,
	title = {A Tutorial on Energy­Based Learning},
	author = {{LeCun}, Yann},
	langid = {english},
	file = {LeCun - A Tutorial on Energy­Based Learning.pdf:C\:\\Users\\foxij\\Zotero\\storage\\6YBVMPSH\\LeCun - A Tutorial on Energy­Based Learning.pdf:application/pdf},
}

@incollection{lavalle_active_2021,
	location = {Cham},
	title = {Active Localization of Multiple Targets from Noisy Relative Measurements},
	volume = {17},
	isbn = {978-3-030-66722-1 978-3-030-66723-8},
	url = {http://link.springer.com/10.1007/978-3-030-66723-8_24},
	abstract = {Consider a mobile robot tasked with localizing targets at unknown locations by obtaining relative measurements. The observations can be bearing or range measurements. How should the robot move so as to localize the targets and minimize the uncertainty in their locations as quickly as possible? This is a diﬃcult optimization problem for which existing approaches are either greedy in nature or they rely on accurate initial estimates.},
	pages = {398--413},
	booktitle = {Algorithmic Foundations of Robotics {XIV}},
	publisher = {Springer International Publishing},
	author = {Engin, Selim and Isler, Volkan},
	editor = {{LaValle}, Steven M. and Lin, Ming and Ojala, Timo and Shell, Dylan and Yu, Jingjin},
	urldate = {2025-02-09},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-66723-8_24},
	note = {Series Title: Springer Proceedings in Advanced Robotics},
	file = {Engin and Isler - 2021 - Active Localization of Multiple Targets from Noisy.pdf:C\:\\Users\\foxij\\Zotero\\storage\\JCWAR9Q7\\Engin and Isler - 2021 - Active Localization of Multiple Targets from Noisy.pdf:application/pdf},
}

@online{noauthor_perspectives_2023,
	title = {Perspectives on diffusion},
	url = {https://sander.ai/2023/07/20/perspectives.html},
	abstract = {Perspectives on diffusion, or how diffusion models are autoencoders, deep latent variable models, score function predictors, reverse {SDE} solvers, flow-based models, {RNNs}, and autoregressive models, all at once!},
	titleaddon = {Sander Dieleman},
	urldate = {2025-02-09},
	date = {2023-07-20},
	langid = {english},
	file = {Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\P2NMMYWF\\perspectives.html:text/html},
}

@misc{song_sliced_2019,
	title = {Sliced Score Matching: A Scalable Approach to Density and Score Estimation},
	url = {http://arxiv.org/abs/1905.07088},
	doi = {10.48550/arXiv.1905.07088},
	shorttitle = {Sliced Score Matching},
	abstract = {Score matching is a popular method for estimating unnormalized statistical models. However, it has been so far limited to simple, shallow models or low-dimensional data, due to the difficulty of computing the Hessian of log-density functions. We show this difficulty can be mitigated by projecting the scores onto random vectors before comparing them. This objective, called sliced score matching, only involves Hessian-vector products, which can be easily implemented using reverse-mode automatic differentiation. Therefore, sliced score matching is amenable to more complex models and higher dimensional data compared to score matching. Theoretically, we prove the consistency and asymptotic normality of sliced score matching estimators. Moreover, we demonstrate that sliced score matching can be used to learn deep score estimators for implicit distributions. In our experiments, we show sliced score matching can learn deep energy-based models effectively, and can produce accurate score estimates for applications such as variational inference with implicit distributions and training Wasserstein Auto-Encoders.},
	number = {{arXiv}:1905.07088},
	publisher = {{arXiv}},
	author = {Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
	urldate = {2025-02-10},
	date = {2019-06-27},
	eprinttype = {arxiv},
	eprint = {1905.07088 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\64H87D6I\\Song et al. - 2019 - Sliced Score Matching A Scalable Approach to Dens.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\MGPASH9Z\\1905.html:text/html},
}

@misc{song_consistency_2023,
	title = {Consistency Models},
	url = {http://arxiv.org/abs/2303.01469},
	doi = {10.48550/arXiv.2303.01469},
	abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art {FID} of 3.55 on {CIFAR}-10 and 6.20 on {ImageNet} 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as {CIFAR}-10, {ImageNet} 64x64 and {LSUN} 256x256.},
	number = {{arXiv}:2303.01469},
	publisher = {{arXiv}},
	author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
	urldate = {2025-02-10},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2303.01469 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\N4GHKH2F\\Song et al. - 2023 - Consistency Models.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\9QLS2LQW\\2303.html:text/html},
}

@misc{yang_diffusion_2024,
	title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
	url = {http://arxiv.org/abs/2209.00796},
	doi = {10.48550/arXiv.2209.00796},
	shorttitle = {Diffusion Models},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/{YangLing}0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	number = {{arXiv}:2209.00796},
	publisher = {{arXiv}},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	urldate = {2025-02-10},
	date = {2024-12-02},
	eprinttype = {arxiv},
	eprint = {2209.00796 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\IJCFZ4JI\\Yang et al. - 2024 - Diffusion Models A Comprehensive Survey of Method.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\PEMHDPTD\\2209.html:text/html},
}

@misc{chen_neural_2019,
	title = {Neural Ordinary Differential Equations},
	url = {http://arxiv.org/abs/1806.07366},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any {ODE} solver, without access to its internal operations. This allows end-to-end training of {ODEs} within larger models.},
	number = {{arXiv}:1806.07366},
	publisher = {{arXiv}},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	urldate = {2025-02-12},
	date = {2019-12-14},
	eprinttype = {arxiv},
	eprint = {1806.07366 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\3XLHURM4\\Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\3BM3HDUN\\1806.html:text/html},
}

@misc{nichol_glide_2022,
	title = {{GLIDE}: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
	url = {http://arxiv.org/abs/2112.10741},
	doi = {10.48550/arXiv.2112.10741},
	shorttitle = {{GLIDE}},
	abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: {CLIP} guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from {DALL}-E, even when the latter uses expensive {CLIP} reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
	number = {{arXiv}:2112.10741},
	publisher = {{arXiv}},
	author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and {McGrew}, Bob and Sutskever, Ilya and Chen, Mark},
	urldate = {2025-02-16},
	date = {2022-03-08},
	eprinttype = {arxiv},
	eprint = {2112.10741 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\LYTABYHF\\Nichol et al. - 2022 - GLIDE Towards Photorealistic Image Generation and.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\6P7J5RV7\\2112.html:text/html},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion Models Beat {GANs} on Image Synthesis},
	url = {http://arxiv.org/abs/2105.05233},
	doi = {10.48550/arXiv.2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an {FID} of 2.97 on {ImageNet} 128\${\textbackslash}times\$128, 4.59 on {ImageNet} 256\${\textbackslash}times\$256, and 7.72 on {ImageNet} 512\${\textbackslash}times\$512, and we match {BigGAN}-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving {FID} to 3.94 on {ImageNet} 256\${\textbackslash}times\$256 and 3.85 on {ImageNet} 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	number = {{arXiv}:2105.05233},
	publisher = {{arXiv}},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	urldate = {2025-02-16},
	date = {2021-06-01},
	eprinttype = {arxiv},
	eprint = {2105.05233 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\NGJF82YY\\Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\Y9CLKWGA\\2105.html:text/html},
}

@misc{nichol_improved_2021,
	title = {Improved Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2102.09672},
	doi = {10.48550/arXiv.2102.09672},
	abstract = {Denoising diffusion probabilistic models ({DDPM}) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, {DDPMs} can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well {DDPMs} and {GANs} cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	number = {{arXiv}:2102.09672},
	publisher = {{arXiv}},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	urldate = {2025-02-16},
	date = {2021-02-18},
	eprinttype = {arxiv},
	eprint = {2102.09672 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\foxij\\Zotero\\storage\\ZHCQ3R6W\\Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf;Snapshot:C\:\\Users\\foxij\\Zotero\\storage\\4SARZXTX\\2102.html:text/html},
}
